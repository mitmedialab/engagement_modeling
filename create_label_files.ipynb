{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c1fce9",
   "metadata": {},
   "source": [
    "# Create lable files \n",
    "annottaions are based on 5s (5\\*30fps)\n",
    "features are based on 64 frames (~2.5s)\n",
    "\n",
    "two things to be done:\n",
    "- devide the lables to two (2.5s)\n",
    "- merge the features to one vector (5s) \n",
    "\n",
    "Lables are set to likart scale of 9\n",
    "- want to try 5 scale and 3 scale too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fda8d",
   "metadata": {},
   "source": [
    "### imports & variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222ca42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "lables_path = './labels/'\n",
    "all_folders_path = './features/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988b1f6",
   "metadata": {},
   "source": [
    "### help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce25d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_labels_regression(dataChange, new_range):\n",
    "    # regression with lables that are minus don't perfom well, so we scale to a new range\n",
    "    min_old_range = -4\n",
    "    max_old_range = 4\n",
    "    if new_range == 5:\n",
    "        min_new_range = 0\n",
    "        max_new_range = 5\n",
    "    elif new_range == 3:\n",
    "        min_new_range = 0\n",
    "        max_new_range = 3\n",
    "    else:\n",
    "        min_new_range = 0\n",
    "        max_new_range = 9\n",
    "        \n",
    "    newdataChange = (((dataChange - min_old_range) * (max_new_range - min_new_range))\n",
    "                     / (max_old_range - min_old_range)) + min_new_range\n",
    "    return newdataChange\n",
    "    \n",
    "def to_five_cat(dataChange):\n",
    "    dataChange.replace(2,1, inplace=True)\n",
    "    dataChange.replace(4,2, inplace=True)\n",
    "    dataChange.replace(3,2, inplace=True)\n",
    "\n",
    "    dataChange.replace(-2,-1, inplace=True)\n",
    "    dataChange.replace(-3,-2, inplace=True)\n",
    "    dataChange.replace(-4,-2, inplace=True)\n",
    "    return dataChange\n",
    "\n",
    "\n",
    "def to_three_cat(dataChange):\n",
    "    dataChange.replace(4,1, inplace=True)\n",
    "    dataChange.replace(3,1, inplace=True)\n",
    "    dataChange.replace(2,1, inplace=True)\n",
    "\n",
    "    dataChange.replace(-2,-1, inplace=True)\n",
    "    dataChange.replace(-3,-1, inplace=True)\n",
    "    dataChange.replace(-4,-1, inplace=True)\n",
    "\n",
    "    return dataChange\n",
    "\n",
    "def to_two_cat(dataChange):\n",
    "    dataChange.replace(4,1, inplace=True)\n",
    "    dataChange.replace(3,1, inplace=True)\n",
    "    dataChange.replace(2,1, inplace=True)\n",
    "\n",
    "    dataChange.replace(-1,0, inplace=True)\n",
    "    dataChange.replace(-2,0, inplace=True)\n",
    "    dataChange.replace(-3,0, inplace=True)\n",
    "    dataChange.replace(-4,0, inplace=True)\n",
    "\n",
    "    return dataChange\n",
    "\n",
    "def person_new_eng_lable(this_file,all_folders_path, eng_lbl, classType, prblemType, classes=9, divide=2.5):\n",
    "    # F10_Interaction_1_P27_rgb.npy <-- remove the _rgb.npy or _flow.npy or vggish etc\n",
    "#     this_bhv = this_file.replace('_'+this_file.split('_')[4],'')\n",
    "    this_bhv = this_file.replace('_high_level.npy','')\n",
    "    engag_lbl = eng_lbl[eng_lbl['behaviour'] == this_bhv]\n",
    "    currData = np.load(os.path.join(all_folders_path,this_file))\n",
    "    \n",
    "    #convert lables to frame-level lables\n",
    "    currLabel = np.repeat(engag_lbl[classType].to_numpy(), repeats=5*30, axis=0)\n",
    "\n",
    "    #... , then merge into the length of the features\n",
    "    reshape_span = int(64*2) if divide == 5 else 64\n",
    "    cutoff = int(currData.shape[0]/2) * reshape_span if divide == 5 else currData.shape[0]*reshape_span\n",
    "    lostLabel = currLabel[cutoff:]\n",
    "    currLabel = currLabel[:cutoff]\n",
    "    \n",
    "    t=currLabel.shape\n",
    "    \n",
    "    if prblemType == 'classification':\n",
    "        currLabel = np.mean(currLabel.reshape(-1, reshape_span), axis=1).round()\n",
    "    elif prblemType == 'regression': # <-- not sure about this for now!\n",
    "        currLabel = np.mean(currLabel.reshape(-1, reshape_span), axis=1)\n",
    "        \n",
    "    #print('{} - {} - {} -> {}'.format(this_file,currData.shape[0],t,currLabel.shape))\n",
    "    \n",
    "    #folder like: classification_eng_lvl_avg_9_2.5s\n",
    "    new_label_folder = '_'.join([classType,'eng_lvl',prblemType,str(classes),str(divide)])\n",
    "    os.makedirs(os.path.join(lables_path,new_label_folder), exist_ok=True)\n",
    "    \n",
    "    new_lable_file = this_bhv+'.npy'\n",
    "    np.save(os.path.join(lables_path,new_label_folder,new_lable_file), currLabel)\n",
    "           \n",
    "\n",
    "def create_new_eng_lable(all_folders_path, classType, prblemType, classes, divide):\n",
    "    eng_lbl = pd.read_csv(os.path.join(lables_path,'Speed_dating_avg_eng.csv'),\n",
    "                          usecols=['video_file','person', classType])\n",
    "    \n",
    "    if classes not in [9,5,3,2]:\n",
    "        sys.exit('The number of classes you selected is not supported')\n",
    "        \n",
    "    if prblemType == 'classification':\n",
    "        if classes == 5:\n",
    "            eng_lbl[classType] = to_five_cat(eng_lbl[classType])\n",
    "        if classes == 3:\n",
    "            eng_lbl[classType] = to_three_cat(eng_lbl[classType])\n",
    "        if classes == 2:\n",
    "            eng_lbl[classType] = to_two_cat(eng_lbl[classType])\n",
    "    elif prblemType == 'regression':\n",
    "        eng_lbl[classType] = normalize_labels_regression(eng_lbl[classType], classes)\n",
    "    \n",
    "    eng_lbl['person'] = pd.DataFrame(eng_lbl['person'].str.split(' ', n=1).tolist())[0]\n",
    "    eng_lbl['behaviour'] = eng_lbl['video_file'].str.cat(eng_lbl['person'], sep=\"_\")\n",
    "\n",
    "    onlyfiles = [f for f in os.listdir(all_folders_path) if\n",
    "                   os.path.isfile(os.path.join(all_folders_path, f))]\n",
    "    onlyfiles.sort()\n",
    "\n",
    "    for this_file in onlyfiles:\n",
    "        person_new_eng_lable(this_file,all_folders_path, eng_lbl, classType, prblemType, classes, divide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778628f2",
   "metadata": {},
   "source": [
    "### create the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd97e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prblemType in ['classification', 'regression']:\n",
    "#     classType = 'round_avg_eng_level' if prblemType == 'classification' else 'avg_eng_level'\n",
    "#     for classes in [9,5,3]:\n",
    "#         for divide in [2.5, 5]:\n",
    "#             print('Working on {}_{}_{}_{}'.format(classType, prblemType, classes, divide))\n",
    "#             create_new_eng_lable(os.path.join(all_folders_path,'i3d_rgb_features'), \n",
    "#                                  classType, prblemType, classes, divide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c925a10",
   "metadata": {},
   "source": [
    "### create file for 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e59254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def person_new_eng_lable_5s(this_file,all_folders_path, eng_lbl, classType, prblemType, classes=9):\n",
    "    # F10_Interaction_1_P27_rgb.npy <-- remove the _rgb.npy or _flow.npy or vggish etc\n",
    "    this_bhv = this_file.replace('_high_level.npy','')\n",
    "    engag_lbl = eng_lbl[eng_lbl['behaviour'] == this_bhv]\n",
    "    \n",
    "    currData = np.load(os.path.join(all_folders_path,this_file))\n",
    "    currLabel = engag_lbl[classType].to_numpy()\n",
    "\n",
    "    tb=currLabel.shape[0]\n",
    "    cutoff = currData.shape[0]\n",
    "    print('{} - {} - {}'.format(this_file,cutoff,tb))\n",
    "    \n",
    "    if tb >= cutoff:\n",
    "        lostLabel = currLabel[cutoff:]\n",
    "        currLabel = currLabel[:cutoff]\n",
    "    else:\n",
    "        lastLabel = currLabel[tb]\n",
    "        di = cutoff - tb\n",
    "        ext_lable = np.repeat(lastLabel,di)\n",
    "        currLabel = np.append(currLabel, ext_lable, axis=0)\n",
    "    \n",
    "    print('{} - {} - {} -> {}'.format(this_file,currData.shape[0],tb,currLabel.shape))\n",
    "    \n",
    "    #folder like: classification_eng_lvl_avg_9_5s\n",
    "    new_label_folder = '_'.join([classType,'eng_lvl',prblemType,str(classes),'5s'])\n",
    "    os.makedirs(os.path.join(lables_path,new_label_folder), exist_ok=True)\n",
    "    \n",
    "    new_lable_file = this_bhv+'.npy'\n",
    "    np.save(os.path.join(lables_path,new_label_folder,new_lable_file), currLabel)\n",
    "           \n",
    "\n",
    "def create_new_eng_lable_5s(all_folders_path, classType, prblemType, classes):\n",
    "    eng_lbl = pd.read_csv(os.path.join(lables_path,'Speed_dating_avg_eng.csv'),\n",
    "                          usecols=['video_file','person', classType])\n",
    "    \n",
    "    if classes not in [9,5,3,2]:\n",
    "        sys.exit('The number of classes you selected is not supported')\n",
    "        \n",
    "    if prblemType == 'classification':\n",
    "        if classes == 5:\n",
    "            eng_lbl[classType] = to_five_cat(eng_lbl[classType])\n",
    "        if classes == 3:\n",
    "            eng_lbl[classType] = to_three_cat(eng_lbl[classType])\n",
    "        if classes == 2:\n",
    "            eng_lbl[classType] = to_two_cat(eng_lbl[classType])\n",
    "    elif prblemType == 'regression':\n",
    "        eng_lbl[classType] = normalize_labels_regression(eng_lbl[classType], classes)\n",
    "    \n",
    "    eng_lbl['person'] = pd.DataFrame(eng_lbl['person'].str.split(' ', n=1).tolist())[0]\n",
    "    eng_lbl['behaviour'] = eng_lbl['video_file'].str.cat(eng_lbl['person'], sep=\"_\")\n",
    "\n",
    "    onlyfiles = [f for f in os.listdir(all_folders_path) if\n",
    "                   os.path.isfile(os.path.join(all_folders_path, f))]\n",
    "    onlyfiles.sort()\n",
    "\n",
    "    for this_file in onlyfiles:\n",
    "        person_new_eng_lable_5s(this_file,all_folders_path, eng_lbl, classType, prblemType, classes)\n",
    "\n",
    "# for prblemType in ['classification', 'regression']:\n",
    "#     classType = 'round_avg_eng_level' if prblemType == 'classification' else 'avg_eng_level'\n",
    "#     for classes in [9,5,3]:\n",
    "#         print('Working on {}_{}_{}'.format(classType, prblemType, classes))\n",
    "#         create_new_eng_lable_5s(os.path.join(all_folders_path,'handcrafted_features_5s'), \n",
    "#                              classType, prblemType, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d66d3",
   "metadata": {},
   "source": [
    "### binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5395714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on round_avg_eng_level_classification_2_2.5\n",
      "Working on round_avg_eng_level_classification_2_5\n",
      "Working on round_avg_eng_level_classification_2_5s\n",
      "F10_Interaction_1_P27_high_level.npy - 213 - 213\n",
      "F10_Interaction_1_P27_high_level.npy - 213 - 213 -> (213,)\n",
      "F10_Interaction_1_P28_high_level.npy - 211 - 213\n",
      "F10_Interaction_1_P28_high_level.npy - 211 - 213 -> (211,)\n",
      "F11_Interaction_1_P29_high_level.npy - 212 - 213\n",
      "F11_Interaction_1_P29_high_level.npy - 212 - 213 -> (212,)\n",
      "F11_Interaction_1_P30_high_level.npy - 212 - 213\n",
      "F11_Interaction_1_P30_high_level.npy - 212 - 213 -> (212,)\n",
      "F11_Interaction_2_P29_high_level.npy - 161 - 213\n",
      "F11_Interaction_2_P29_high_level.npy - 161 - 213 -> (161,)\n",
      "F11_Interaction_2_P30_high_level.npy - 184 - 213\n",
      "F11_Interaction_2_P30_high_level.npy - 184 - 213 -> (184,)\n",
      "F13_Interaction_1_P32_high_level.npy - 213 - 213\n",
      "F13_Interaction_1_P32_high_level.npy - 213 - 213 -> (213,)\n",
      "F13_Interaction_1_P33_high_level.npy - 213 - 213\n",
      "F13_Interaction_1_P33_high_level.npy - 213 - 213 -> (213,)\n",
      "F17_Interaction_1_P37_high_level.npy - 178 - 213\n",
      "F17_Interaction_1_P37_high_level.npy - 178 - 213 -> (178,)\n",
      "F17_Interaction_1_P38_high_level.npy - 204 - 213\n",
      "F17_Interaction_1_P38_high_level.npy - 204 - 213 -> (204,)\n",
      "F17_Interaction_2_P37_high_level.npy - 162 - 213\n",
      "F17_Interaction_2_P37_high_level.npy - 162 - 213 -> (162,)\n",
      "F17_Interaction_2_P38_high_level.npy - 167 - 213\n",
      "F17_Interaction_2_P38_high_level.npy - 167 - 213 -> (167,)\n",
      "F1_Interaction_1_P1_high_level.npy - 213 - 213\n",
      "F1_Interaction_1_P1_high_level.npy - 213 - 213 -> (213,)\n",
      "F1_Interaction_1_P2_high_level.npy - 212 - 213\n",
      "F1_Interaction_1_P2_high_level.npy - 212 - 213 -> (212,)\n",
      "F1_Interaction_1_P3_high_level.npy - 212 - 213\n",
      "F1_Interaction_1_P3_high_level.npy - 212 - 213 -> (212,)\n",
      "F1_Interaction_2_P1_high_level.npy - 209 - 213\n",
      "F1_Interaction_2_P1_high_level.npy - 209 - 213 -> (209,)\n",
      "F1_Interaction_2_P2_high_level.npy - 213 - 213\n",
      "F1_Interaction_2_P2_high_level.npy - 213 - 213 -> (213,)\n",
      "F1_Interaction_2_P3_high_level.npy - 114 - 213\n",
      "F1_Interaction_2_P3_high_level.npy - 114 - 213 -> (114,)\n",
      "F2_Interaction_1_P4_high_level.npy - 213 - 213\n",
      "F2_Interaction_1_P4_high_level.npy - 213 - 213 -> (213,)\n",
      "F2_Interaction_1_P5_high_level.npy - 213 - 213\n",
      "F2_Interaction_1_P5_high_level.npy - 213 - 213 -> (213,)\n",
      "F2_Interaction_2_P4_high_level.npy - 74 - 78\n",
      "F2_Interaction_2_P4_high_level.npy - 74 - 78 -> (74,)\n",
      "F3_Interaction_1_P6_high_level.npy - 213 - 213\n",
      "F3_Interaction_1_P6_high_level.npy - 213 - 213 -> (213,)\n",
      "F3_Interaction_1_P7_high_level.npy - 213 - 213\n",
      "F3_Interaction_1_P7_high_level.npy - 213 - 213 -> (213,)\n",
      "F3_Interaction_1_P8_high_level.npy - 129 - 213\n",
      "F3_Interaction_1_P8_high_level.npy - 129 - 213 -> (129,)\n",
      "F3_Interaction_2_P6_high_level.npy - 212 - 213\n",
      "F3_Interaction_2_P6_high_level.npy - 212 - 213 -> (212,)\n",
      "F3_Interaction_2_P7_high_level.npy - 206 - 213\n",
      "F3_Interaction_2_P7_high_level.npy - 206 - 213 -> (206,)\n",
      "F4_Interaction_1_P10_high_level.npy - 212 - 213\n",
      "F4_Interaction_1_P10_high_level.npy - 212 - 213 -> (212,)\n",
      "F4_Interaction_1_P11_high_level.npy - 213 - 213\n",
      "F4_Interaction_1_P11_high_level.npy - 213 - 213 -> (213,)\n",
      "F4_Interaction_1_P12_high_level.npy - 213 - 213\n",
      "F4_Interaction_1_P12_high_level.npy - 213 - 213 -> (213,)\n",
      "F4_Interaction_1_P13_high_level.npy - 161 - 213\n",
      "F4_Interaction_1_P13_high_level.npy - 161 - 213 -> (161,)\n",
      "F4_Interaction_1_P14_high_level.npy - 169 - 213\n",
      "F4_Interaction_1_P14_high_level.npy - 169 - 213 -> (169,)\n",
      "F4_Interaction_1_P9_high_level.npy - 195 - 213\n",
      "F4_Interaction_1_P9_high_level.npy - 195 - 213 -> (195,)\n",
      "F4_Interaction_2_P10_high_level.npy - 213 - 213\n",
      "F4_Interaction_2_P10_high_level.npy - 213 - 213 -> (213,)\n",
      "F4_Interaction_2_P11_high_level.npy - 208 - 213\n",
      "F4_Interaction_2_P11_high_level.npy - 208 - 213 -> (208,)\n",
      "F4_Interaction_2_P12_high_level.npy - 206 - 213\n",
      "F4_Interaction_2_P12_high_level.npy - 206 - 213 -> (206,)\n",
      "F4_Interaction_2_P9_high_level.npy - 209 - 213\n",
      "F4_Interaction_2_P9_high_level.npy - 209 - 213 -> (209,)\n",
      "F5_Interaction_1_P15_high_level.npy - 209 - 213\n",
      "F5_Interaction_1_P15_high_level.npy - 209 - 213 -> (209,)\n",
      "F5_Interaction_1_P16_high_level.npy - 212 - 213\n",
      "F5_Interaction_1_P16_high_level.npy - 212 - 213 -> (212,)\n",
      "F5_Interaction_2_P15_high_level.npy - 213 - 213\n",
      "F5_Interaction_2_P15_high_level.npy - 213 - 213 -> (213,)\n",
      "F5_Interaction_2_P16_high_level.npy - 213 - 213\n",
      "F5_Interaction_2_P16_high_level.npy - 213 - 213 -> (213,)\n",
      "F6_Interaction_1_P17_high_level.npy - 213 - 213\n",
      "F6_Interaction_1_P17_high_level.npy - 213 - 213 -> (213,)\n",
      "F6_Interaction_1_P18_high_level.npy - 213 - 213\n",
      "F6_Interaction_1_P18_high_level.npy - 213 - 213 -> (213,)\n",
      "F6_Interaction_1_P19_high_level.npy - 213 - 213\n",
      "F6_Interaction_1_P19_high_level.npy - 213 - 213 -> (213,)\n",
      "F6_Interaction_2_P17_high_level.npy - 213 - 213\n",
      "F6_Interaction_2_P17_high_level.npy - 213 - 213 -> (213,)\n",
      "F6_Interaction_2_P18_high_level.npy - 213 - 213\n",
      "F6_Interaction_2_P18_high_level.npy - 213 - 213 -> (213,)\n",
      "F7_Interaction_1_P20_high_level.npy - 201 - 213\n",
      "F7_Interaction_1_P20_high_level.npy - 201 - 213 -> (201,)\n",
      "F7_Interaction_1_P21_high_level.npy - 211 - 213\n",
      "F7_Interaction_1_P21_high_level.npy - 211 - 213 -> (211,)\n",
      "F7_Interaction_1_P22_high_level.npy - 213 - 213\n",
      "F7_Interaction_1_P22_high_level.npy - 213 - 213 -> (213,)\n",
      "F7_Interaction_1_P23_high_level.npy - 200 - 213\n",
      "F7_Interaction_1_P23_high_level.npy - 200 - 213 -> (200,)\n",
      "F8_Interaction_1_P24_high_level.npy - 206 - 213\n",
      "F8_Interaction_1_P24_high_level.npy - 206 - 213 -> (206,)\n",
      "F8_Interaction_1_P25_high_level.npy - 208 - 213\n",
      "F8_Interaction_1_P25_high_level.npy - 208 - 213 -> (208,)\n",
      "F8_Interaction_2_P24_high_level.npy - 195 - 213\n",
      "F8_Interaction_2_P24_high_level.npy - 195 - 213 -> (195,)\n",
      "F8_Interaction_2_P25_high_level.npy - 213 - 213\n",
      "F8_Interaction_2_P25_high_level.npy - 213 - 213 -> (213,)\n",
      "F8_Interaction_3_P24_high_level.npy - 211 - 213\n",
      "F8_Interaction_3_P24_high_level.npy - 211 - 213 -> (211,)\n",
      "F8_Interaction_3_P25_high_level.npy - 207 - 213\n",
      "F8_Interaction_3_P25_high_level.npy - 207 - 213 -> (207,)\n"
     ]
    }
   ],
   "source": [
    "for prblemType in ['classification']:\n",
    "    classType = 'round_avg_eng_level' if prblemType == 'classification' else 'avg_eng_level'\n",
    "    for classes in [2]:\n",
    "        for divide in [2.5, 5, '5s']:\n",
    "            print('Working on {}_{}_{}_{}'.format(classType, prblemType, classes, str(divide)))\n",
    "            if divide in [2.5, 5]:\n",
    "                create_new_eng_lable(os.path.join(all_folders_path,'handcrafted_features_'+str(divide)), \n",
    "                                     classType, prblemType, classes, divide)\n",
    "            elif divide == '5s':\n",
    "                create_new_eng_lable_5s(os.path.join(all_folders_path,'handcrafted_features_5s'), \n",
    "                             classType, prblemType, classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
