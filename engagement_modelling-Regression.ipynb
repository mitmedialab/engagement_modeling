{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e42fc71",
   "metadata": {},
   "source": [
    "# Modlling for engagement \n",
    "\n",
    "now we have the features and the lables, we are ready for modelling \n",
    "- Classification vs regression\n",
    "- 9 vs 5 vs 3 scales lables \n",
    "- 2.5 vs 5 s window (as is features, average featuers and concatenate features)\n",
    "- Two-stream Fusion on RGB + Flow \n",
    "\n",
    "This should be done over:\n",
    "- different network artchictures\n",
    "- cross different familys cross-validation \n",
    "\n",
    "Later on:\n",
    "- Handcrafted features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243c294",
   "metadata": {},
   "source": [
    "### imports and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dacd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "import resreg\n",
    "\n",
    "lables_path = './labels/'\n",
    "features_path = './features/'\n",
    "results_path = './modelling_results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2f46d",
   "metadata": {},
   "source": [
    "### Keras modeling class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cebbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "class KerasWorker(Worker):\n",
    "    def __init__(self, input_shape, output_shape, problemType,\n",
    "                 x_train, y_train, x_validation, y_validation,\n",
    "                 x_test, y_test, shared_directory, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.input_shape = (input_shape, )\n",
    "            self.num_classes = output_shape\n",
    "            self.batch_size = 64\n",
    "            self.save_dic = shared_directory\n",
    "            \n",
    "            self.problemType = problemType\n",
    "\n",
    "            self.x_train, self.y_train = x_train, y_train\n",
    "            self.x_validation, self.y_validation = x_validation, y_validation\n",
    "            self.x_test, self.y_test = x_test, y_test\n",
    "\n",
    "    def compute(self, config, budget, working_directory, *args, **kwargs):\n",
    "            model = Sequential()\n",
    "            model.add(Dense(units=config['start_neurons_units'],\n",
    "                            # activation=config['start_neurons_activation'],\n",
    "                            activation='relu',\n",
    "                            input_shape=self.input_shape))\n",
    "\n",
    "\n",
    "            if config['num_dense_layers'] > 1:\n",
    "                model.add(Dense(units=config['dense1_units'],\n",
    "                                # activation=config['dense1_activation'],\n",
    "                                activation='relu',\n",
    "                                input_shape=self.input_shape))\n",
    "                model.add(Dropout(config['dropout1_rate']))\n",
    "\n",
    "            if config['num_dense_layers'] > 2:\n",
    "                model.add(Dense(units=config['dense2_units'],\n",
    "                                # activation=config['dense2_activation'],\n",
    "                                activation='relu',\n",
    "                                input_shape=self.input_shape))\n",
    "                model.add(Dropout(config['dropout2_rate']))\n",
    "\n",
    "            model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "            if config['optimizer'] == 'Adam':\n",
    "                    optimizer = tf.keras.optimizers.Adam(lr=config['lr'])\n",
    "            else:\n",
    "                    optimizer = tf.keras.optimizers.SGD(lr=config['lr'], momentum=config['sgd_momentum'])\n",
    "            \n",
    "            loss_fn = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\")\n",
    "            \n",
    "            METRICS = [\n",
    "                tf.keras.losses.MeanAbsoluteError(reduction=\"auto\", name=\"mae\"),\n",
    "                tf.keras.losses.MeanAbsolutePercentageError(reduction=\"auto\", name=\"maperc_e\"),\n",
    "                tf.keras.losses.MeanSquaredLogarithmicError(reduction=\"auto\", name=\"mslog_e\"),\n",
    "                tf.keras.losses.CosineSimilarity(reduction=\"auto\", name=\"CosineSimilarity\"),\n",
    "                tf.keras.losses.Huber(reduction=\"auto\", name=\"Huber\"),\n",
    "                tf.keras.losses.LogCosh(reduction=\"auto\", name=\"LogCosh\"),\n",
    "            ]\n",
    "\n",
    "            #METRICS.append(tfa.metrics.MatthewsCorrelationCoefficient(num_classes=self.num_classes))\n",
    "            \n",
    "            the_columns = ['loss','mae','maperc_e','mslog_e','CosineSimilarity','Huber','LogCosh']\n",
    "            \n",
    "            model.compile(\n",
    "                loss=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                metrics=METRICS\n",
    "            )\n",
    "\n",
    "            # model.summary()\n",
    "            _history = model.fit(self.x_train, self.y_train,\n",
    "                              batch_size=self.batch_size,\n",
    "                              epochs=int(budget),\n",
    "                              verbose=0,\n",
    "                              validation_data=(self.x_validation, self.y_validation))\n",
    "\n",
    "            val_loss_per_epoch = _history.history['val_loss']\n",
    "            best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
    "\n",
    "            model.fit(self.x_train, self.y_train,\n",
    "                              batch_size=self.batch_size,\n",
    "                              epochs=best_epoch,\n",
    "                              verbose=0,\n",
    "                              validation_data=(self.x_validation, self.y_validation))\n",
    "\n",
    "            train_score = model.evaluate(self.x_train, self.y_train, verbose=0)\n",
    "            val_score = model.evaluate(self.x_validation, self.y_validation, verbose=0)\n",
    "            test_score = model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "            \n",
    "            #print(test_score)\n",
    "\n",
    "            resultsDF = pd.DataFrame([train_score,val_score,test_score],\n",
    "                                     columns=the_columns,\n",
    "                                     index=[\"train_score\", \"val_score\", \"test_score\"],)\n",
    "            # print(resultsDF)\n",
    "            test_predictions_baseline = model.predict(self.x_test)\n",
    "            np.savetxt(os.path.join(self.save_dic,'testing_finalResults_true.out'), self.y_test, delimiter=',')\n",
    "            np.savetxt(os.path.join(self.save_dic,'testing_finalResults_pred.out'), test_predictions_baseline, delimiter=',')\n",
    "\n",
    "            return ({\n",
    "                'loss': test_score[0],  \n",
    "                'info':  resultsDF.to_dict('index')\n",
    "            })\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_configspace():\n",
    "            \"\"\"\n",
    "            It builds the configuration space with the needed hyperparameters.\n",
    "            It is easily possible to implement different types of hyperparameters.\n",
    "            Beside float-hyperparameters on a log scale, it is also able to handle categorical input parameter.\n",
    "            :return: ConfigurationsSpace-Object\n",
    "            \"\"\"\n",
    "            cs = CS.ConfigurationSpace()\n",
    "\n",
    "            lr = CSH.UniformFloatHyperparameter('lr', lower=1e-6, upper=1e-1, default_value='1e-2', log=True)\n",
    "\n",
    "            # For demonstration purposes, we add different optimizers as categorical hyperparameters.\n",
    "            # To show how to use conditional hyperparameters with ConfigSpace, we'll add the optimizers 'Adam' and 'SGD'.\n",
    "            # SGD has a different parameter 'momentum'.\n",
    "            optimizer = CSH.CategoricalHyperparameter('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "            sgd_momentum = CSH.UniformFloatHyperparameter('sgd_momentum', lower=0.0, upper=0.99, default_value=0.9, log=False)\n",
    "\n",
    "            cs.add_hyperparameters([lr, optimizer, sgd_momentum])\n",
    "\n",
    "\n",
    "\n",
    "            num_dense_layers =  CSH.UniformIntegerHyperparameter('num_dense_layers', lower=1, upper=3, default_value=2)\n",
    "\n",
    "            start_neurons_units = CSH.UniformIntegerHyperparameter('start_neurons_units', lower=32, upper=512, default_value=32, log=True)\n",
    "            dense1_units = CSH.UniformIntegerHyperparameter('dense1_units', lower=8, upper=128, default_value=16, log=True)\n",
    "            dense2_units = CSH.UniformIntegerHyperparameter('dense2_units', lower=4, upper=64, default_value=8, log=True)\n",
    "\n",
    "            cs.add_hyperparameters([num_dense_layers, start_neurons_units, dense1_units, dense2_units])\n",
    "\n",
    "            # start_neurons_activation = CSH.CategoricalHyperparameter('start_neurons_activation', ['relu', 'tanh', 'sigmoid'])\n",
    "            # dense1_activation = CSH.CategoricalHyperparameter('dense1_activation', ['relu', 'tanh', 'sigmoid'])\n",
    "            # dense2_activation = CSH.CategoricalHyperparameter('dense2_activation', ['relu', 'tanh', 'sigmoid'])\n",
    "            # start_neurons_activation = CSH.CategoricalHyperparameter('start_neurons_activation', ['relu'])\n",
    "            # dense1_activation = CSH.CategoricalHyperparameter('dense1_activation', ['relu'])\n",
    "            # dense2_activation = CSH.CategoricalHyperparameter('dense2_activation', ['relu'])\n",
    "            #\n",
    "            # cs.add_hyperparameters([start_neurons_activation, dense1_activation, dense2_activation])\n",
    "\n",
    "            dropout1_rate = CSH.UniformFloatHyperparameter('dropout1_rate', lower=0.0, upper=0.9, default_value=0.5, log=False)\n",
    "            dropout2_rate = CSH.UniformFloatHyperparameter('dropout2_rate', lower=0.0, upper=0.9, default_value=0.5, log=False)\n",
    "\n",
    "            cs.add_hyperparameters([dropout1_rate, dropout2_rate])\n",
    "\n",
    "\n",
    "            # The hyperparameter sgd_momentum will be used,if the configuration\n",
    "            # contains 'SGD' as optimizer.\n",
    "            cond = CS.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            # You can also use inequality conditions:\n",
    "            cond = CS.GreaterThanCondition(dense1_units, num_dense_layers, 1)\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            cond = CS.GreaterThanCondition(dense2_units, num_dense_layers, 2)\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            return cs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83f4f9",
   "metadata": {},
   "source": [
    "### help functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac1b61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sub_dataset_twostream(familiesSet, label_folder, feature_folder):\n",
    "    # append all rows of subjects, and their lables\n",
    "    allFrames = np.array([])\n",
    "    allLables = np.array([])\n",
    "    \n",
    "    rgb_feature_folder = feature_folder.replace(featureType,'rgb')\n",
    "    flow_feature_folder = feature_folder.replace(featureType,'flow')\n",
    "    for this_family in familiesSet:\n",
    "        onlyfiles = [f for f in os.listdir(rgb_feature_folder) if\n",
    "                       os.path.isfile(os.path.join(rgb_feature_folder, f))\n",
    "                       and f.startswith(this_family + '_')]\n",
    "        onlyfiles.sort()\n",
    "\n",
    "        for this_file in onlyfiles:\n",
    "            this_lable_file = this_file.replace('_'+this_file.split('_')[4],'')+'.npy'\n",
    "            currLabel = np.load(os.path.join(label_folder,this_lable_file))\n",
    "            \n",
    "            rgb_currData = np.load(os.path.join(rgb_feature_folder,this_file))    \n",
    "            flow_currData = np.load(os.path.join(flow_feature_folder,this_file.replace('rgb','flow')))\n",
    "            \n",
    "            currData = np.hstack((rgb_currData, flow_currData))\n",
    "            \n",
    "            if allFrames.shape[0] ==0:\n",
    "                allFrames = currData\n",
    "                allLables = currLabel\n",
    "            else:\n",
    "                allFrames = np.vstack((allFrames, currData))\n",
    "                allLables = np.hstack((allLables, currLabel))\n",
    "\n",
    "    return allFrames, allLables\n",
    "\n",
    "def load_sub_dataset(familiesSet, label_folder, feature_folder, featureType):\n",
    "    # append all rows of subjects, and their lables\n",
    "    allFrames = np.array([])\n",
    "    allLables = np.array([])\n",
    "    \n",
    "    if featureType == 'twostream':\n",
    "        return load_sub_dataset_twostream(familiesSet, label_folder, feature_folder)\n",
    "    \n",
    "    for this_family in familiesSet:\n",
    "        # F10_Interaction_1_P27_rgb.npy\n",
    "        onlyfiles = [f for f in os.listdir(feature_folder) if\n",
    "                       os.path.isfile(os.path.join(feature_folder, f))\n",
    "                       and f.startswith(this_family + '_')]\n",
    "        onlyfiles.sort()\n",
    "\n",
    "        for this_file in onlyfiles:\n",
    "            currData = np.load(os.path.join(feature_folder,this_file))\n",
    "            substr = '_'.join(this_file.split('_')[4:])\n",
    "            this_lable_file = this_file.replace('_'+substr,'')+'.npy'\n",
    "            currLabel = np.load(os.path.join(label_folder,this_lable_file))\n",
    "            cutoff = min(len(currData), len(currLabel))\n",
    "            currData = currData[:cutoff,:]\n",
    "            currLabel =currLabel[:cutoff]\n",
    "            \n",
    "            if allFrames.shape[0] ==0:\n",
    "                allFrames = currData\n",
    "                allLables = currLabel\n",
    "            else:\n",
    "                allFrames = np.vstack((allFrames, currData))\n",
    "                allLables = np.hstack((allLables, currLabel))\n",
    "\n",
    "    return allFrames, allLables\n",
    "\n",
    "def regression_random_oversampling(dataX, datay):\n",
    "    low_channel_percentile = 10\n",
    "    high_channel_percentile = 90\n",
    "    relevance_threshold_percentile = 95 \n",
    "       \n",
    "    relevance = resreg.sigmoid_relevance(datay, cl=np.percentile(datay,low_channel_percentile),\\\n",
    "                                         ch=np.percentile(datay,high_channel_percentile))\n",
    "    dataX, datay = resreg.random_oversample(dataX ,datay, relevance, \n",
    "                                relevance_threshold=np.percentile(relevance,relevance_threshold_percentile),\\\n",
    "                                over='balance', random_state=0)\n",
    "    return dataX, datay\n",
    "\n",
    "\n",
    "def load_dataset_selectedSubj(trainSubjs, valSubjs, testSubjs, label_folder, feature_folder,\\\n",
    "                              prblemType, featureType, num_classes):\n",
    "    #simple sampeling method for refression\n",
    "    #TODO: advanced ones SMOTER? <-- does it work for deep features \n",
    "    \n",
    "    # load all train\n",
    "    trainX, trainy = load_sub_dataset(trainSubjs, label_folder, feature_folder, featureType)\n",
    "    trainX, trainy = shuffle(trainX, trainy)\n",
    "#     _ = plt.hist(trainy, bins='auto')\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    # Resample training data (Random Oversampling)   \n",
    "    trainX, trainy = regression_random_oversampling(trainX, trainy)\n",
    "#     _ = plt.hist(trainy, bins='auto')\n",
    "#     plt.show()\n",
    " \n",
    "    # load validation\n",
    "    valX, valy = load_sub_dataset(valSubjs, label_folder, feature_folder, featureType)\n",
    "    valX, valy = shuffle(valX, valy)\n",
    "#     _ = plt.hist(valy, bins='auto')\n",
    "#     plt.show()\n",
    "    \n",
    "    # Resample training data (Random Oversampling)\n",
    "    valX, valy = regression_random_oversampling(valX ,valy)\n",
    "#     _ = plt.hist(valy, bins='auto')\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    # load all test\n",
    "    testX, testy = load_sub_dataset(testSubjs, label_folder, feature_folder, featureType)\n",
    "#     _ = plt.hist(testy, bins='auto')\n",
    "#     plt.show()\n",
    "    \n",
    "    # Resample training data (Random Oversampling)\n",
    "    testX, testy = regression_random_oversampling(testX ,testy)\n",
    "#     _ = plt.hist(testy, bins='auto')\n",
    "#     plt.show()\n",
    "\n",
    "    return trainX, trainy, valX, valy , testX, testy, num_classes\n",
    "    \n",
    "def create_modlling(label_folder,feature_folder,result_folder, prblemType, featureType, num_classes):    \n",
    "    # repeat experiment\n",
    "    temp = {}\n",
    "    all_trainSubjs = [['F' + str(i) for i in [1, 2, 3, 4, 5, 6, 8]]]\n",
    "    all_valSubjs = [['F' + str(i) for i in [11, 17]]]\n",
    "    all_testSubjs = [['F' + str(i) for i in [7, 10, 13]]]\n",
    "    \n",
    "    min_budget = 9\n",
    "    max_budget = 243\n",
    "    n_iterations = 50\n",
    "    num_workers = 12\n",
    "\n",
    "    for r in range(len(all_trainSubjs)):\n",
    "        shared_directory = result_folder + '_'+ str(r) \n",
    "        if os.path.exists(shared_directory):\n",
    "            print(shared_directory,' already processed')\n",
    "            continue\n",
    "        \n",
    "        print(shared_directory,' under processing')\n",
    "        classType = os.path.basename(shared_directory)\n",
    "\n",
    "        # load data\n",
    "        trainSubjs = all_trainSubjs[r]\n",
    "        valSubjs = all_valSubjs[r]\n",
    "        testSubjs = all_testSubjs[r]\n",
    "        \n",
    "        trainX, trainy, valX, valy, testX, testy, num_classes = \\\n",
    "            load_dataset_selectedSubj(trainSubjs, valSubjs, testSubjs, \\\n",
    "                                      label_folder, feature_folder, prblemType, featureType, num_classes)\n",
    "        \n",
    "        \n",
    "        n_timesteps, n_features, n_outputs = trainX.shape[0], trainX.shape[1], num_classes        \n",
    "        \n",
    "        host = hpns.nic_name_to_host('lo')\n",
    "        result_logger = hpres.json_result_logger(directory=shared_directory, overwrite=True)\n",
    "        NS = hpns.NameServer(run_id=classType, host=host, port=0, working_directory=shared_directory)\n",
    "        ns_host, ns_port = NS.start()\n",
    "    \n",
    "        workers = []\n",
    "        for i in range(num_workers):\n",
    "            worker = KerasWorker(n_features, n_outputs, prblemType, \\\n",
    "                                 trainX, trainy, valX, valy, testX, testy, \\\n",
    "                                 shared_directory,\n",
    "                                 run_id=classType,host=host, nameserver=ns_host, nameserver_port=ns_port,\n",
    "                                 id=i)\n",
    "            worker.run(background=True)\n",
    "            workers.append(worker)\n",
    "\n",
    "        bohb = BOHB(configspace=worker.get_configspace(),\n",
    "                  run_id=classType,\n",
    "                  host=host,\n",
    "                  nameserver=ns_host,\n",
    "                  nameserver_port=ns_port,\n",
    "                  result_logger=result_logger,\n",
    "                  min_budget=min_budget, max_budget=max_budget\n",
    "                    )\n",
    "        res = bohb.run(n_iterations=1,  min_n_workers=num_workers)\n",
    "\n",
    "        id2config = res.get_id2config_mapping()\n",
    "        incumbent = res.get_incumbent_id()\n",
    "\n",
    "        print('Best found configuration:', id2config[incumbent]['config'])\n",
    "        # print('A total of %i unique configurations where sampled.' % len(id2config.keys()))\n",
    "        # print('A total of %i runs where executed.' % len(res.get_all_runs()))\n",
    "        # print('Total budget corresponds to %.1f full function evaluations.' % (\n",
    "        #             sum([r.budget for r in res.get_all_runs()]) / max_budget))\n",
    "\n",
    "        # store results\n",
    "        with open(os.path.join(shared_directory, 'results.pkl'), 'wb') as fh:\n",
    "            pickle.dump(res, fh)\n",
    "\n",
    "        # shutdown\n",
    "        bohb.shutdown(shutdown_workers=True)\n",
    "        NS.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b9729",
   "metadata": {},
   "source": [
    "### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9e58a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "Working on:  regression rgb 9 5 conc\n",
      "./modelling_results/rgb_regression_9_5_conc_0  under processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 18:08:05.020575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-28 18:08:05.048305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sharifa/catkin_ws/devel/lib:/usr/local/cuda-11.0/lib64\n",
      "2021-10-28 18:08:05.050392: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sharifa/catkin_ws/devel/lib:/usr/local/cuda-11.0/lib64\n",
      "2021-10-28 18:08:05.050406: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-28 18:08:05.051328: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/sharifa/engagement_modeling/eng_env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-10-28 18:08:06.371055: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best found configuration: {'dropout1_rate': 0.7287920973258931, 'dropout2_rate': 0.10169571820335176, 'lr': 0.0013457546042931275, 'num_dense_layers': 2, 'optimizer': 'Adam', 'start_neurons_units': 82, 'dense1_units': 71}\n",
      "Working on:  regression rgb 9 5 avg\n",
      "./modelling_results/rgb_regression_9_5_avg_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.48646939605159456, 'dropout2_rate': 0.589148418545302, 'lr': 0.000989323144392342, 'num_dense_layers': 2, 'optimizer': 'Adam', 'start_neurons_units': 34, 'dense1_units': 125}\n",
      "Working on:  regression rgb 9 2.5 none\n",
      "./modelling_results/rgb_regression_9_2.5_none_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.4421659173280363, 'dropout2_rate': 0.7767571774296114, 'lr': 2.4617823583142346e-05, 'num_dense_layers': 2, 'optimizer': 'Adam', 'start_neurons_units': 481, 'dense1_units': 95}\n",
      "Working on:  regression rgb 5 5 conc\n",
      "./modelling_results/rgb_regression_5_5_conc_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.40102258336088137, 'dropout2_rate': 0.18823169616679475, 'lr': 0.003141368338345185, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 152, 'dense1_units': 65, 'dense2_units': 4}\n",
      "Working on:  regression rgb 5 5 avg\n",
      "./modelling_results/rgb_regression_5_5_avg_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.8010993262029216, 'dropout2_rate': 0.7634644600202423, 'lr': 0.004415246663892426, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 100, 'dense1_units': 53, 'dense2_units': 45}\n",
      "Working on:  regression rgb 5 2.5 none\n",
      "./modelling_results/rgb_regression_5_2.5_none_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.8465897445372323, 'dropout2_rate': 0.5078545050147396, 'lr': 0.012082374832941742, 'num_dense_layers': 1, 'optimizer': 'Adam', 'start_neurons_units': 56}\n",
      "Working on:  regression rgb 3 5 conc\n",
      "./modelling_results/rgb_regression_3_5_conc_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.6894016527753878, 'dropout2_rate': 0.6318902946731427, 'lr': 9.003251041654563e-05, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 51, 'dense1_units': 97, 'dense2_units': 27}\n",
      "Working on:  regression rgb 3 5 avg\n",
      "./modelling_results/rgb_regression_3_5_avg_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.668412651431825, 'dropout2_rate': 0.43905718130383453, 'lr': 0.00016849100155794715, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 317, 'dense1_units': 22, 'dense2_units': 12}\n",
      "Working on:  regression rgb 3 2.5 none\n",
      "./modelling_results/rgb_regression_3_2.5_none_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.8669624794340771, 'dropout2_rate': 0.3573531084266474, 'lr': 0.002472014738414124, 'num_dense_layers': 1, 'optimizer': 'Adam', 'start_neurons_units': 46}\n",
      "Working on:  regression flow 9 5 conc\n",
      "./modelling_results/flow_regression_9_5_conc_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.28334604513552325, 'dropout2_rate': 0.47837527528725393, 'lr': 0.04146905408870019, 'num_dense_layers': 3, 'optimizer': 'SGD', 'start_neurons_units': 103, 'dense1_units': 89, 'dense2_units': 4, 'sgd_momentum': 0.5724973151858603}\n",
      "Working on:  regression flow 9 5 avg\n",
      "./modelling_results/flow_regression_9_5_avg_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.6955009117174069, 'dropout2_rate': 0.6114709877102317, 'lr': 4.587512346272294e-05, 'num_dense_layers': 1, 'optimizer': 'Adam', 'start_neurons_units': 200}\n",
      "Working on:  regression flow 9 2.5 none\n",
      "./modelling_results/flow_regression_9_2.5_none_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.018785655355968458, 'dropout2_rate': 0.21717276210164047, 'lr': 0.004109773247987596, 'num_dense_layers': 1, 'optimizer': 'Adam', 'start_neurons_units': 185}\n",
      "Working on:  regression flow 5 5 conc\n",
      "./modelling_results/flow_regression_5_5_conc_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.01735535511458083, 'dropout2_rate': 0.6989027568437612, 'lr': 7.822683855667587e-05, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 113, 'dense1_units': 35, 'dense2_units': 7}\n",
      "Working on:  regression flow 5 5 avg\n",
      "./modelling_results/flow_regression_5_5_avg_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.2922782599909263, 'dropout2_rate': 0.020130105589173688, 'lr': 0.002594775647505975, 'num_dense_layers': 1, 'optimizer': 'Adam', 'start_neurons_units': 129}\n",
      "Working on:  regression flow 5 2.5 none\n",
      "./modelling_results/flow_regression_5_2.5_none_0  under processing\n",
      "WARNING:tensorflow:5 out of the last 28 calls to <function Model.make_train_function.<locals>.train_function at 0x7f475435a9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 28 calls to <function Model.make_train_function.<locals>.train_function at 0x7f475435a9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 29 calls to <function Model.make_train_function.<locals>.train_function at 0x7f47e077caf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 29 calls to <function Model.make_train_function.<locals>.train_function at 0x7f47e077caf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best found configuration: {'dropout1_rate': 0.354374452638158, 'dropout2_rate': 0.8828550716490757, 'lr': 0.05355249552914864, 'num_dense_layers': 3, 'optimizer': 'SGD', 'start_neurons_units': 55, 'dense1_units': 126, 'dense2_units': 13, 'sgd_momentum': 0.4173627396248238}\n",
      "Working on:  regression flow 3 5 conc\n",
      "./modelling_results/flow_regression_3_5_conc_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.8181061805775379, 'dropout2_rate': 0.8010675370169473, 'lr': 0.0003030256397544131, 'num_dense_layers': 2, 'optimizer': 'Adam', 'start_neurons_units': 393, 'dense1_units': 67}\n",
      "Working on:  regression flow 3 5 avg\n",
      "./modelling_results/flow_regression_3_5_avg_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.8201169125459854, 'dropout2_rate': 0.30858676613825786, 'lr': 0.0003068900660178044, 'num_dense_layers': 2, 'optimizer': 'Adam', 'start_neurons_units': 76, 'dense1_units': 89}\n",
      "Working on:  regression flow 3 2.5 none\n",
      "./modelling_results/flow_regression_3_2.5_none_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.43849048138645724, 'dropout2_rate': 0.7488237943495691, 'lr': 7.802106690776835e-05, 'num_dense_layers': 2, 'optimizer': 'Adam', 'start_neurons_units': 37, 'dense1_units': 36}\n",
      "Working on:  regression twostream 9 5 conc\n",
      "./modelling_results/twostream_regression_9_5_conc_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.37330890210575607, 'dropout2_rate': 0.050054195951038175, 'lr': 0.00302299290441476, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 103, 'dense1_units': 24, 'dense2_units': 41}\n",
      "Working on:  regression twostream 9 5 avg\n",
      "./modelling_results/twostream_regression_9_5_avg_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.3486281014373396, 'dropout2_rate': 0.8533000772663806, 'lr': 0.00020228620773356647, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 36, 'dense1_units': 10, 'dense2_units': 52}\n",
      "Working on:  regression twostream 9 2.5 none\n",
      "./modelling_results/twostream_regression_9_2.5_none_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.47214995538849786, 'dropout2_rate': 0.754883916393257, 'lr': 0.03186328290738405, 'num_dense_layers': 2, 'optimizer': 'SGD', 'start_neurons_units': 282, 'dense1_units': 38, 'sgd_momentum': 0.5983729411230075}\n",
      "Working on:  regression twostream 5 5 conc\n",
      "./modelling_results/twostream_regression_5_5_conc_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.6990315254923416, 'dropout2_rate': 0.4350466492331552, 'lr': 0.03807462817690775, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 59, 'dense1_units': 62, 'dense2_units': 5}\n",
      "Working on:  regression twostream 5 5 avg\n",
      "./modelling_results/twostream_regression_5_5_avg_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.16422287017581852, 'dropout2_rate': 0.5249634206473766, 'lr': 0.03508892119449954, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 304, 'dense1_units': 31, 'dense2_units': 8}\n",
      "Working on:  regression twostream 5 2.5 none\n",
      "./modelling_results/twostream_regression_5_2.5_none_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.5487371882688724, 'dropout2_rate': 0.62884209829553, 'lr': 0.024309240350086605, 'num_dense_layers': 2, 'optimizer': 'Adam', 'start_neurons_units': 72, 'dense1_units': 22}\n",
      "Working on:  regression twostream 3 5 conc\n",
      "./modelling_results/twostream_regression_3_5_conc_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.7983443755516135, 'dropout2_rate': 0.6272202691536524, 'lr': 0.09110676394861586, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 203, 'dense1_units': 9, 'dense2_units': 8}\n",
      "Working on:  regression twostream 3 5 avg\n",
      "./modelling_results/twostream_regression_3_5_avg_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.4225642065063275, 'dropout2_rate': 0.6815549392211413, 'lr': 0.006246072593154952, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 172, 'dense1_units': 31, 'dense2_units': 24}\n",
      "Working on:  regression twostream 3 2.5 none\n",
      "./modelling_results/twostream_regression_3_2.5_none_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.6294445360846644, 'dropout2_rate': 0.40915292793818386, 'lr': 1.3241477235064742e-05, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 47, 'dense1_units': 12, 'dense2_units': 13}\n"
     ]
    }
   ],
   "source": [
    "prblemTypes = ['regression']\n",
    "featureTypes = ['rgb','flow', 'twostream']\n",
    "classes = [9,5,3]\n",
    "# divides = [2.5, 5]\n",
    "fusionTypes = ['conc','avg','none']\n",
    "\n",
    "permutations=[ prblemTypes, featureTypes, classes, fusionTypes]\n",
    "all_permutations = list(itertools.product(*permutations))\n",
    "print(len(all_permutations))\n",
    "for this_permutation in all_permutations:\n",
    "    (prblemType, featureType, eng_lvls, fusionType) = this_permutation\n",
    "    classType = 'round_avg_eng_level' if prblemType == 'classification' else 'avg_eng_level'\n",
    "    divide = 2.5 if fusionType == 'none' else 5\n",
    "    \n",
    "    print('Working on: ',prblemType, featureType, eng_lvls, divide, fusionType)\n",
    "    \n",
    "    label_folder = os.path.join(lables_path,'_'.join([classType,'eng_lvl',prblemType,str(eng_lvls),str(divide)]))\n",
    "\n",
    "    \n",
    "    if divide == 5:\n",
    "        extra_txt = '_'.join([fusionType,str(divide)]) \n",
    "        feature_folder = os.path.join(features_path,'_'.join(['i3d',featureType,'features',extra_txt]))\n",
    "    else:\n",
    "        feature_folder = os.path.join(features_path,'_'.join(['i3d',featureType,'features']))\n",
    "        \n",
    "    \n",
    "    #save results like: rgb_classification_9_2.5_none\n",
    "    result_folder = os.path.join(results_path,'_'.join([featureType,prblemType,str(eng_lvls),str(divide),fusionType]))\n",
    "\n",
    "    create_modlling(label_folder,feature_folder,result_folder, prblemType, featureType, eng_lvls)#, divide,fusionType)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd273b",
   "metadata": {},
   "source": [
    "### modelling for handcrafted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4d627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Working on:  regression handcrafted 9 2.5\n",
      "./modelling_results/handcrafted_regression_9_2.5_0  under processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 00:31:49.815459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-03 00:31:49.825765: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sharifa/catkin_ws/devel/lib:/usr/local/cuda-11.0/lib64\n",
      "2021-11-03 00:31:49.826875: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sharifa/catkin_ws/devel/lib:/usr/local/cuda-11.0/lib64\n",
      "2021-11-03 00:31:49.826896: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-11-03 00:31:49.828860: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/sharifa/engagement_modeling/eng_env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-11-03 00:31:50.869785: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best found configuration: {'dropout1_rate': 0.06928502265657847, 'dropout2_rate': 0.242897575655007, 'lr': 1.3705353174132053e-06, 'num_dense_layers': 2, 'optimizer': 'SGD', 'start_neurons_units': 61, 'dense1_units': 24, 'sgd_momentum': 0.7006122059958867}\n",
      "Working on:  regression handcrafted 9 5\n",
      "./modelling_results/handcrafted_regression_9_5_0  under processing\n",
      "Best found configuration: {'dropout1_rate': 0.46257912355785086, 'dropout2_rate': 0.33783306560160464, 'lr': 0.005038575427971224, 'num_dense_layers': 3, 'optimizer': 'Adam', 'start_neurons_units': 44, 'dense1_units': 126, 'dense2_units': 9}\n",
      "Working on:  regression handcrafted 9 5s\n",
      "./modelling_results/handcrafted_regression_9_5s_0  under processing\n"
     ]
    }
   ],
   "source": [
    "prblemTypes = ['regression']\n",
    "featureTypes = ['handcrafted']\n",
    "classes = [9,5,3]\n",
    "# divides = [2.5, 5]\n",
    "fusionTypes = ['2.5','5','5s']\n",
    "\n",
    "permutations=[ prblemTypes, featureTypes, classes, fusionTypes]\n",
    "all_permutations = list(itertools.product(*permutations))\n",
    "print(len(all_permutations))\n",
    "for this_permutation in all_permutations:\n",
    "    (prblemType, featureType, eng_lvls, fusionType) = this_permutation\n",
    "    classType = 'round_avg_eng_level' if prblemType == 'classification' else 'avg_eng_level'\n",
    "    \n",
    "    print('Working on: ',prblemType, featureType, eng_lvls, fusionType)\n",
    "    \n",
    "    label_folder = os.path.join(lables_path,'_'.join([classType,'eng_lvl',prblemType,str(eng_lvls),str(fusionType)]))\n",
    "    feature_folder = os.path.join(features_path,'_'.join([featureType,'features',fusionType]))\n",
    "        \n",
    "    #save results like: rgb_classification_9_2.5_none\n",
    "    result_folder = os.path.join(results_path,'_'.join([featureType,prblemType,str(eng_lvls),fusionType]))\n",
    "\n",
    "    create_modlling(label_folder,feature_folder,result_folder, prblemType, featureType, eng_lvls)#, divide,fusionType)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ab503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
